{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ed42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde1ed8",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 1     # Choose step of the hyperparameter tuning \n",
    "MODES = {'1':'Base_Model', '2':'Gray_Scale', '3':'Experience_Replay', '4':'Frame_Skipping', '5':'Max_Pooling', '6':'Update_Frequency'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9d368",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "__credits__ = [\"Andrea PIERRÉ\"]\n",
    "\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.box2d.car_dynamics import Car\n",
    "from gym.error import DependencyNotInstalled, InvalidAction\n",
    "from gym.utils import EzPickle\n",
    "from gym.utils.renderer import Renderer\n",
    "\n",
    "try:\n",
    "    import Box2D\n",
    "    from Box2D.b2 import contactListener, fixtureDef, polygonShape\n",
    "except ImportError:\n",
    "    raise DependencyNotInstalled(\"box2D is not installed, run `pip install gym[box2d]`\")\n",
    "\n",
    "\n",
    "STATE_W = 96  # less than Atari 160x192\n",
    "STATE_H = 96\n",
    "VIDEO_W = 600\n",
    "VIDEO_H = 400\n",
    "WINDOW_W = 1000\n",
    "WINDOW_H = 800\n",
    "\n",
    "SCALE = 6.0  # Track scale\n",
    "TRACK_RAD = 900 / SCALE  # Track is heavily morphed circle with this radius\n",
    "PLAYFIELD = 2000 / SCALE  # Game over boundary\n",
    "FPS = 50  # Frames per second\n",
    "ZOOM = 2.7  # Camera zoom\n",
    "ZOOM_FOLLOW = True  # Set to False for fixed view (don't use zoom)\n",
    "\n",
    "\n",
    "TRACK_DETAIL_STEP = 21 / SCALE\n",
    "TRACK_TURN_RATE = 0.31\n",
    "TRACK_WIDTH = 40 / SCALE\n",
    "BORDER = 8 / SCALE\n",
    "BORDER_MIN_COUNT = 4\n",
    "GRASS_DIM = PLAYFIELD / 20.0\n",
    "MAX_SHAPE_DIM = (\n",
    "    max(GRASS_DIM, TRACK_WIDTH, TRACK_DETAIL_STEP) * math.sqrt(2) * ZOOM * SCALE\n",
    ")\n",
    "\n",
    "\n",
    "class FrictionDetector(contactListener):\n",
    "    def __init__(self, env, lap_complete_percent):\n",
    "        contactListener.__init__(self)\n",
    "        self.env = env\n",
    "        self.lap_complete_percent = lap_complete_percent\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        self._contact(contact, True)\n",
    "\n",
    "    def EndContact(self, contact):\n",
    "        self._contact(contact, False)\n",
    "\n",
    "    def _contact(self, contact, begin):\n",
    "        tile = None\n",
    "        obj = None\n",
    "        u1 = contact.fixtureA.body.userData\n",
    "        u2 = contact.fixtureB.body.userData\n",
    "        if u1 and \"road_friction\" in u1.__dict__:\n",
    "            tile = u1\n",
    "            obj = u2\n",
    "        if u2 and \"road_friction\" in u2.__dict__:\n",
    "            tile = u2\n",
    "            obj = u1\n",
    "        if not tile:\n",
    "            return\n",
    "\n",
    "        # inherit tile color from env\n",
    "        tile.color = self.env.road_color / 255\n",
    "        if not obj or \"tiles\" not in obj.__dict__:\n",
    "            return\n",
    "        if begin:\n",
    "            obj.tiles.add(tile)\n",
    "            if not tile.road_visited:\n",
    "                tile.road_visited = True\n",
    "                self.env.reward += 1000.0 / len(self.env.track)\n",
    "                self.env.tile_visited_count += 1\n",
    "\n",
    "                # Lap is considered completed if enough % of the track was covered\n",
    "                if (\n",
    "                    tile.idx == 0\n",
    "                    and self.env.tile_visited_count / len(self.env.track)\n",
    "                    > self.lap_complete_percent\n",
    "                ):\n",
    "                    self.env.new_lap = True\n",
    "        else:\n",
    "            obj.tiles.remove(tile)\n",
    "\n",
    "\n",
    "class CarRacing(gym.Env, EzPickle):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    The easiest control task to learn from pixels - a top-down\n",
    "    racing environment. The generated track is random every episode.\n",
    "    Some indicators are shown at the bottom of the window along with the\n",
    "    state RGB buffer. From left to right: true speed, four ABS sensors,\n",
    "    steering wheel position, and gyroscope.\n",
    "    To play yourself (it's rather fast for humans), type:\n",
    "    ```\n",
    "    python gym/envs/box2d/car_racing.py\n",
    "    ```\n",
    "    Remember: it's a powerful rear-wheel drive car - don't press the accelerator\n",
    "    and turn at the same time.\n",
    "    ### Action Space\n",
    "    There are 3 actions: steering (-1 is full left, +1 is full right), gas,\n",
    "    and breaking.\n",
    "    ### Observation Space\n",
    "    State consists of 96x96 pixels.\n",
    "    ### Rewards\n",
    "    The reward is -0.1 every frame and +1000/N for every track tile visited,\n",
    "    where N is the total number of tiles visited in the track. For example,\n",
    "    if you have finished in 732 frames, your reward is\n",
    "    1000 - 0.1*732 = 926.8 points.\n",
    "    ### Starting State\n",
    "    The car starts at rest in the center of the road.\n",
    "    ### Episode Termination\n",
    "    The episode finishes when all of the tiles are visited. The car can also go\n",
    "    outside of the playfield - that is, far off the track, in which case it will\n",
    "    receive -100 reward and die.\n",
    "    ### Arguments\n",
    "    `lap_complete_percent` dictates the percentage of tiles that must be visited by\n",
    "    the agent before a lap is considered complete.\n",
    "    Passing `domain_randomize=True` enables the domain randomized variant of the environment.\n",
    "    In this scenario, the background and track colours are different on every reset.\n",
    "    Passing `continuous=False` converts the environment to use discrete action space.\n",
    "    The discrete action space has 5 actions: [do nothing, left, right, gas, brake].\n",
    "    ### Version History\n",
    "    - v1: Change track completion logic and add domain randomization (0.24.0)\n",
    "    - v0: Original version\n",
    "    ### References\n",
    "    - Chris Campbell (2014), http://www.iforce2d.net/b2dtut/top-down-car.\n",
    "    ### Credits\n",
    "    Created by Oleg Klimov\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"state_pixels\",\n",
    "            \"single_rgb_array\",\n",
    "            \"single_state_pixels\",\n",
    "        ],\n",
    "        \"render_fps\": FPS,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        render_mode: Optional[str] = None,\n",
    "        verbose: bool = False,\n",
    "        lap_complete_percent: float = 0.95,\n",
    "        domain_randomize: bool = False,\n",
    "        continuous: bool = False,\n",
    "    ):\n",
    "        EzPickle.__init__(self)\n",
    "        self.continuous = continuous\n",
    "        self.domain_randomize = domain_randomize\n",
    "        self._init_colors()\n",
    "\n",
    "        self.contactListener_keepref = FrictionDetector(self, lap_complete_percent)\n",
    "        self.world = Box2D.b2World((0, 0), contactListener=self.contactListener_keepref)\n",
    "        self.screen = None\n",
    "        self.surf = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.invisible_state_window = None\n",
    "        self.invisible_video_window = None\n",
    "        self.road = None\n",
    "        self.car = None\n",
    "        self.reward = 0.0\n",
    "        self.prev_reward = 0.0\n",
    "        self.verbose = verbose\n",
    "        self.new_lap = False\n",
    "        self.fd_tile = fixtureDef(\n",
    "            shape=polygonShape(vertices=[(0, 0), (1, 0), (1, -1), (0, -1)])\n",
    "        )\n",
    "\n",
    "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
    "        #   or normalised however this is not possible here so ignore\n",
    "        if self.continuous:\n",
    "            self.action_space = spaces.Box(\n",
    "                np.array([-1, 0, 0]).astype(np.float32),\n",
    "                np.array([+1, +1, +1]).astype(np.float32),\n",
    "            )  # steer, gas, brake\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(5)\n",
    "            # do nothing, left, right, gas, brake\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(STATE_H, STATE_W, 3), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.renderer = Renderer(self.render_mode, self._render)\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.road:\n",
    "            return\n",
    "        for t in self.road:\n",
    "            self.world.DestroyBody(t)\n",
    "        self.road = []\n",
    "        self.car.destroy()\n",
    "\n",
    "    def _init_colors(self):\n",
    "        if self.domain_randomize:\n",
    "            # domain randomize the bg and grass colour\n",
    "            self.road_color = self.np_random.uniform(0, 210, size=3)\n",
    "\n",
    "            self.bg_color = self.np_random.uniform(0, 210, size=3)\n",
    "\n",
    "            self.grass_color = np.copy(self.bg_color)\n",
    "            idx = self.np_random.integers(3)\n",
    "            self.grass_color[idx] += 20\n",
    "        else:\n",
    "            # default colours\n",
    "            self.road_color = np.array([102, 102, 102])\n",
    "            self.bg_color = np.array([102, 204, 102])\n",
    "            self.grass_color = np.array([102, 230, 102])\n",
    "\n",
    "    def _create_track(self):\n",
    "        CHECKPOINTS = 12\n",
    "\n",
    "        # Create checkpoints\n",
    "        checkpoints = []\n",
    "        for c in range(CHECKPOINTS):\n",
    "            noise = self.np_random.uniform(0, 2 * math.pi * 1 / CHECKPOINTS)\n",
    "            alpha = 2 * math.pi * c / CHECKPOINTS + noise\n",
    "            rad = self.np_random.uniform(TRACK_RAD / 3, TRACK_RAD)\n",
    "\n",
    "            if c == 0:\n",
    "                alpha = 0\n",
    "                rad = 1.5 * TRACK_RAD\n",
    "            if c == CHECKPOINTS - 1:\n",
    "                alpha = 2 * math.pi * c / CHECKPOINTS\n",
    "                self.start_alpha = 2 * math.pi * (-0.5) / CHECKPOINTS\n",
    "                rad = 1.5 * TRACK_RAD\n",
    "\n",
    "            checkpoints.append((alpha, rad * math.cos(alpha), rad * math.sin(alpha)))\n",
    "        self.road = []\n",
    "\n",
    "        # Go from one checkpoint to another to create track\n",
    "        x, y, beta = 1.5 * TRACK_RAD, 0, 0\n",
    "        dest_i = 0\n",
    "        laps = 0\n",
    "        track = []\n",
    "        no_freeze = 2500\n",
    "        visited_other_side = False\n",
    "        while True:\n",
    "            alpha = math.atan2(y, x)\n",
    "            if visited_other_side and alpha > 0:\n",
    "                laps += 1\n",
    "                visited_other_side = False\n",
    "            if alpha < 0:\n",
    "                visited_other_side = True\n",
    "                alpha += 2 * math.pi\n",
    "\n",
    "            while True:  # Find destination from checkpoints\n",
    "                failed = True\n",
    "\n",
    "                while True:\n",
    "                    dest_alpha, dest_x, dest_y = checkpoints[dest_i % len(checkpoints)]\n",
    "                    if alpha <= dest_alpha:\n",
    "                        failed = False\n",
    "                        break\n",
    "                    dest_i += 1\n",
    "                    if dest_i % len(checkpoints) == 0:\n",
    "                        break\n",
    "\n",
    "                if not failed:\n",
    "                    break\n",
    "\n",
    "                alpha -= 2 * math.pi\n",
    "                continue\n",
    "\n",
    "            r1x = math.cos(beta)\n",
    "            r1y = math.sin(beta)\n",
    "            p1x = -r1y\n",
    "            p1y = r1x\n",
    "            dest_dx = dest_x - x  # vector towards destination\n",
    "            dest_dy = dest_y - y\n",
    "            # destination vector projected on rad:\n",
    "            proj = r1x * dest_dx + r1y * dest_dy\n",
    "            while beta - alpha > 1.5 * math.pi:\n",
    "                beta -= 2 * math.pi\n",
    "            while beta - alpha < -1.5 * math.pi:\n",
    "                beta += 2 * math.pi\n",
    "            prev_beta = beta\n",
    "            proj *= SCALE\n",
    "            if proj > 0.3:\n",
    "                beta -= min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
    "            if proj < -0.3:\n",
    "                beta += min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
    "            x += p1x * TRACK_DETAIL_STEP\n",
    "            y += p1y * TRACK_DETAIL_STEP\n",
    "            track.append((alpha, prev_beta * 0.5 + beta * 0.5, x, y))\n",
    "            if laps > 4:\n",
    "                break\n",
    "            no_freeze -= 1\n",
    "            if no_freeze == 0:\n",
    "                break\n",
    "\n",
    "        # Find closed loop range i1..i2, first loop should be ignored, second is OK\n",
    "        i1, i2 = -1, -1\n",
    "        i = len(track)\n",
    "        while True:\n",
    "            i -= 1\n",
    "            if i == 0:\n",
    "                return False  # Failed\n",
    "            pass_through_start = (\n",
    "                track[i][0] > self.start_alpha and track[i - 1][0] <= self.start_alpha\n",
    "            )\n",
    "            if pass_through_start and i2 == -1:\n",
    "                i2 = i\n",
    "            elif pass_through_start and i1 == -1:\n",
    "                i1 = i\n",
    "                break\n",
    "        if self.verbose:\n",
    "            print(\"Track generation: %i..%i -> %i-tiles track\" % (i1, i2, i2 - i1))\n",
    "        assert i1 != -1\n",
    "        assert i2 != -1\n",
    "\n",
    "        track = track[i1 : i2 - 1]\n",
    "\n",
    "        first_beta = track[0][1]\n",
    "        first_perp_x = math.cos(first_beta)\n",
    "        first_perp_y = math.sin(first_beta)\n",
    "        # Length of perpendicular jump to put together head and tail\n",
    "        well_glued_together = np.sqrt(\n",
    "            np.square(first_perp_x * (track[0][2] - track[-1][2]))\n",
    "            + np.square(first_perp_y * (track[0][3] - track[-1][3]))\n",
    "        )\n",
    "        if well_glued_together > TRACK_DETAIL_STEP:\n",
    "            return False\n",
    "\n",
    "        # Red-white border on hard turns\n",
    "        border = [False] * len(track)\n",
    "        for i in range(len(track)):\n",
    "            good = True\n",
    "            oneside = 0\n",
    "            for neg in range(BORDER_MIN_COUNT):\n",
    "                beta1 = track[i - neg - 0][1]\n",
    "                beta2 = track[i - neg - 1][1]\n",
    "                good &= abs(beta1 - beta2) > TRACK_TURN_RATE * 0.2\n",
    "                oneside += np.sign(beta1 - beta2)\n",
    "            good &= abs(oneside) == BORDER_MIN_COUNT\n",
    "            border[i] = good\n",
    "        for i in range(len(track)):\n",
    "            for neg in range(BORDER_MIN_COUNT):\n",
    "                border[i - neg] |= border[i]\n",
    "\n",
    "        # Create tiles\n",
    "        for i in range(len(track)):\n",
    "            alpha1, beta1, x1, y1 = track[i]\n",
    "            alpha2, beta2, x2, y2 = track[i - 1]\n",
    "            road1_l = (\n",
    "                x1 - TRACK_WIDTH * math.cos(beta1),\n",
    "                y1 - TRACK_WIDTH * math.sin(beta1),\n",
    "            )\n",
    "            road1_r = (\n",
    "                x1 + TRACK_WIDTH * math.cos(beta1),\n",
    "                y1 + TRACK_WIDTH * math.sin(beta1),\n",
    "            )\n",
    "            road2_l = (\n",
    "                x2 - TRACK_WIDTH * math.cos(beta2),\n",
    "                y2 - TRACK_WIDTH * math.sin(beta2),\n",
    "            )\n",
    "            road2_r = (\n",
    "                x2 + TRACK_WIDTH * math.cos(beta2),\n",
    "                y2 + TRACK_WIDTH * math.sin(beta2),\n",
    "            )\n",
    "            vertices = [road1_l, road1_r, road2_r, road2_l]\n",
    "            self.fd_tile.shape.vertices = vertices\n",
    "            t = self.world.CreateStaticBody(fixtures=self.fd_tile)\n",
    "            t.userData = t\n",
    "            c = 0.01 * (i % 3) * 255\n",
    "            t.color = self.road_color + c\n",
    "            t.road_visited = False\n",
    "            t.road_friction = 1.0\n",
    "            t.idx = i\n",
    "            t.fixtures[0].sensor = True\n",
    "            self.road_poly.append(([road1_l, road1_r, road2_r, road2_l], t.color))\n",
    "            self.road.append(t)\n",
    "            if border[i]:\n",
    "                side = np.sign(beta2 - beta1)\n",
    "                b1_l = (\n",
    "                    x1 + side * TRACK_WIDTH * math.cos(beta1),\n",
    "                    y1 + side * TRACK_WIDTH * math.sin(beta1),\n",
    "                )\n",
    "                b1_r = (\n",
    "                    x1 + side * (TRACK_WIDTH + BORDER) * math.cos(beta1),\n",
    "                    y1 + side * (TRACK_WIDTH + BORDER) * math.sin(beta1),\n",
    "                )\n",
    "                b2_l = (\n",
    "                    x2 + side * TRACK_WIDTH * math.cos(beta2),\n",
    "                    y2 + side * TRACK_WIDTH * math.sin(beta2),\n",
    "                )\n",
    "                b2_r = (\n",
    "                    x2 + side * (TRACK_WIDTH + BORDER) * math.cos(beta2),\n",
    "                    y2 + side * (TRACK_WIDTH + BORDER) * math.sin(beta2),\n",
    "                )\n",
    "                self.road_poly.append(\n",
    "                    (\n",
    "                        [b1_l, b1_r, b2_r, b2_l],\n",
    "                        (255, 255, 255) if i % 2 == 0 else (255, 0, 0),\n",
    "                    )\n",
    "                )\n",
    "        self.track = track\n",
    "        return True\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        return_info: bool = False,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self._destroy()\n",
    "        self.reward = 0.0\n",
    "        self.prev_reward = 0.0\n",
    "        self.tile_visited_count = 0\n",
    "        self.t = 0.0\n",
    "        self.new_lap = False\n",
    "        self.road_poly = []\n",
    "        self._init_colors()\n",
    "\n",
    "        while True:\n",
    "            success = self._create_track()\n",
    "            if success:\n",
    "                break\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"retry to generate track (normal if there are not many\"\n",
    "                    \"instances of this message)\"\n",
    "                )\n",
    "        self.car = Car(self.world, *self.track[0][1:4])\n",
    "\n",
    "        self.renderer.reset()\n",
    "        if not return_info:\n",
    "            return self.step(None)[0]\n",
    "        else:\n",
    "            return self.step(None)[0], {}\n",
    "\n",
    "    def step(self, action: Union[np.ndarray, int]):\n",
    "        if action is not None:\n",
    "            if self.continuous:\n",
    "                self.car.steer(-action[0])\n",
    "                self.car.gas(action[1])\n",
    "                self.car.brake(action[2])\n",
    "            else:\n",
    "                if not self.action_space.contains(action):\n",
    "                    raise InvalidAction(\n",
    "                        f\"you passed the invalid action `{action}`. \"\n",
    "                        f\"The supported action_space is `{self.action_space}`\"\n",
    "                    )\n",
    "                self.car.steer(-0.6 * (action == 1) + 0.6 * (action == 2))\n",
    "                self.car.gas(0.2 * (action == 3))\n",
    "                self.car.brake(0.8 * (action == 4))\n",
    "\n",
    "        self.car.step(1.0 / FPS)\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
    "        self.t += 1.0 / FPS\n",
    "\n",
    "        self.state = self._render(\"single_state_pixels\")\n",
    "\n",
    "        step_reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "        if action is not None:  # First step without action, called from reset()\n",
    "            self.reward -= 0.1\n",
    "            # We actually don't want to count fuel spent, we want car to be faster.\n",
    "            # self.reward -=  10 * self.car.fuel_spent / ENGINE_POWER\n",
    "            self.car.fuel_spent = 0.0\n",
    "            step_reward = self.reward - self.prev_reward\n",
    "            self.prev_reward = self.reward\n",
    "            if self.tile_visited_count == len(self.track) or self.new_lap:\n",
    "                done = True\n",
    "                # Termination due to finishing lap\n",
    "                # This should not be treated as a failure\n",
    "                # but like a timeout\n",
    "                info[\"TimeLimit.truncated\"] = True\n",
    "            x, y = self.car.hull.position\n",
    "            if abs(x) > PLAYFIELD or abs(y) > PLAYFIELD:\n",
    "                done = True\n",
    "                step_reward = -100\n",
    "\n",
    "        self.renderer.render_step()\n",
    "        return self.state, step_reward, done, info\n",
    "\n",
    "    def render(self, mode: str = \"human\"):\n",
    "        if self.render_mode is not None:\n",
    "            return self.renderer.get_renders()\n",
    "        else:\n",
    "            return self._render(mode)\n",
    "\n",
    "    def _render(self, mode: str = \"human\"):\n",
    "        assert mode in self.metadata[\"render_modes\"]\n",
    "        try:\n",
    "            import pygame\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[box2d]`\"\n",
    "            )\n",
    "\n",
    "        pygame.font.init()\n",
    "\n",
    "        if self.screen is None and mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((WINDOW_W, WINDOW_H))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        if \"t\" not in self.__dict__:\n",
    "            return  # reset() not called yet\n",
    "\n",
    "        self.surf = pygame.Surface((WINDOW_W, WINDOW_H))\n",
    "\n",
    "        # computing transformations\n",
    "        angle = -self.car.hull.angle\n",
    "        # Animating first second zoom.\n",
    "        zoom = 0.1 * SCALE * max(1 - self.t, 0) + ZOOM * SCALE * min(self.t, 1)\n",
    "        scroll_x = -(self.car.hull.position[0]) * zoom\n",
    "        scroll_y = -(self.car.hull.position[1]) * zoom\n",
    "        trans = pygame.math.Vector2((scroll_x, scroll_y)).rotate_rad(angle)\n",
    "        trans = (WINDOW_W / 2 + trans[0], WINDOW_H / 4 + trans[1])\n",
    "\n",
    "        self._render_road(zoom, trans, angle)\n",
    "        self.car.draw(\n",
    "            self.surf,\n",
    "            zoom,\n",
    "            trans,\n",
    "            angle,\n",
    "            mode not in [\"state_pixels\", \"single_state_pixels\"],\n",
    "        )\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "\n",
    "        # showing stats\n",
    "        self._render_indicators(WINDOW_W, WINDOW_H)\n",
    "\n",
    "        font = pygame.font.Font(pygame.font.get_default_font(), 42)\n",
    "        text = font.render(\"%04i\" % self.reward, True, (255, 255, 255), (0, 0, 0))\n",
    "        text_rect = text.get_rect()\n",
    "        text_rect.center = (60, WINDOW_H - WINDOW_H * 2.5 / 40.0)\n",
    "        self.surf.blit(text, text_rect)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            self.screen.fill(0)\n",
    "            self.screen.blit(self.surf, (0, 0))\n",
    "            pygame.display.flip()\n",
    "\n",
    "        if mode in {\"rgb_array\", \"single_rgb_array\"}:\n",
    "            return self._create_image_array(self.surf, (VIDEO_W, VIDEO_H))\n",
    "        elif mode in {\"state_pixels\", \"single_state_pixels\"}:\n",
    "            return self._create_image_array(self.surf, (STATE_W, STATE_H))\n",
    "        else:\n",
    "            return self.isopen\n",
    "\n",
    "    def _render_road(self, zoom, translation, angle):\n",
    "        bounds = PLAYFIELD\n",
    "        field = [\n",
    "            (bounds, bounds),\n",
    "            (bounds, -bounds),\n",
    "            (-bounds, -bounds),\n",
    "            (-bounds, bounds),\n",
    "        ]\n",
    "\n",
    "        # draw background\n",
    "        self._draw_colored_polygon(\n",
    "            self.surf, field, self.bg_color, zoom, translation, angle, clip=False\n",
    "        )\n",
    "\n",
    "        # draw grass patches\n",
    "        grass = []\n",
    "        for x in range(-20, 20, 2):\n",
    "            for y in range(-20, 20, 2):\n",
    "                grass.append(\n",
    "                    [\n",
    "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + 0),\n",
    "                        (GRASS_DIM * x + 0, GRASS_DIM * y + 0),\n",
    "                        (GRASS_DIM * x + 0, GRASS_DIM * y + GRASS_DIM),\n",
    "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + GRASS_DIM),\n",
    "                    ]\n",
    "                )\n",
    "        for poly in grass:\n",
    "            self._draw_colored_polygon(\n",
    "                self.surf, poly, self.grass_color, zoom, translation, angle\n",
    "            )\n",
    "\n",
    "        # draw road\n",
    "        for poly, color in self.road_poly:\n",
    "            # converting to pixel coordinates\n",
    "            poly = [(p[0], p[1]) for p in poly]\n",
    "            color = [int(c) for c in color]\n",
    "            self._draw_colored_polygon(self.surf, poly, color, zoom, translation, angle)\n",
    "\n",
    "    def _render_indicators(self, W, H):\n",
    "        import pygame\n",
    "\n",
    "        s = W / 40.0\n",
    "        h = H / 40.0\n",
    "        color = (0, 0, 0)\n",
    "        polygon = [(W, H), (W, H - 5 * h), (0, H - 5 * h), (0, H)]\n",
    "        pygame.draw.polygon(self.surf, color=color, points=polygon)\n",
    "\n",
    "        def vertical_ind(place, val):\n",
    "            return [\n",
    "                (place * s, H - (h + h * val)),\n",
    "                ((place + 1) * s, H - (h + h * val)),\n",
    "                ((place + 1) * s, H - h),\n",
    "                ((place + 0) * s, H - h),\n",
    "            ]\n",
    "\n",
    "        def horiz_ind(place, val):\n",
    "            return [\n",
    "                ((place + 0) * s, H - 4 * h),\n",
    "                ((place + val) * s, H - 4 * h),\n",
    "                ((place + val) * s, H - 2 * h),\n",
    "                ((place + 0) * s, H - 2 * h),\n",
    "            ]\n",
    "\n",
    "        true_speed = np.sqrt(\n",
    "            np.square(self.car.hull.linearVelocity[0])\n",
    "            + np.square(self.car.hull.linearVelocity[1])\n",
    "        )\n",
    "\n",
    "        # simple wrapper to render if the indicator value is above a threshold\n",
    "        def render_if_min(value, points, color):\n",
    "            if abs(value) > 1e-4:\n",
    "                pygame.draw.polygon(self.surf, points=points, color=color)\n",
    "\n",
    "        render_if_min(true_speed, vertical_ind(5, 0.02 * true_speed), (255, 255, 255))\n",
    "        # ABS sensors\n",
    "        render_if_min(\n",
    "            self.car.wheels[0].omega,\n",
    "            vertical_ind(7, 0.01 * self.car.wheels[0].omega),\n",
    "            (0, 0, 255),\n",
    "        )\n",
    "        render_if_min(\n",
    "            self.car.wheels[1].omega,\n",
    "            vertical_ind(8, 0.01 * self.car.wheels[1].omega),\n",
    "            (0, 0, 255),\n",
    "        )\n",
    "        render_if_min(\n",
    "            self.car.wheels[2].omega,\n",
    "            vertical_ind(9, 0.01 * self.car.wheels[2].omega),\n",
    "            (51, 0, 255),\n",
    "        )\n",
    "        render_if_min(\n",
    "            self.car.wheels[3].omega,\n",
    "            vertical_ind(10, 0.01 * self.car.wheels[3].omega),\n",
    "            (51, 0, 255),\n",
    "        )\n",
    "\n",
    "        render_if_min(\n",
    "            self.car.wheels[0].joint.angle,\n",
    "            horiz_ind(20, -10.0 * self.car.wheels[0].joint.angle),\n",
    "            (0, 255, 0),\n",
    "        )\n",
    "        render_if_min(\n",
    "            self.car.hull.angularVelocity,\n",
    "            horiz_ind(30, -0.8 * self.car.hull.angularVelocity),\n",
    "            (255, 0, 0),\n",
    "        )\n",
    "\n",
    "    def _draw_colored_polygon(\n",
    "        self, surface, poly, color, zoom, translation, angle, clip=True\n",
    "    ):\n",
    "        import pygame\n",
    "        from pygame import gfxdraw\n",
    "\n",
    "        poly = [pygame.math.Vector2(c).rotate_rad(angle) for c in poly]\n",
    "        poly = [\n",
    "            (c[0] * zoom + translation[0], c[1] * zoom + translation[1]) for c in poly\n",
    "        ]\n",
    "        # This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\n",
    "        # Instead of calculating exactly if the polygon and screen overlap,\n",
    "        # we simply check if the polygon is in a larger bounding box whose dimension\n",
    "        # is greater than the screen by MAX_SHAPE_DIM, which is the maximum\n",
    "        # diagonal length of an environment object\n",
    "        if not clip or any(\n",
    "            (-MAX_SHAPE_DIM <= coord[0] <= WINDOW_W + MAX_SHAPE_DIM)\n",
    "            and (-MAX_SHAPE_DIM <= coord[1] <= WINDOW_H + MAX_SHAPE_DIM)\n",
    "            for coord in poly\n",
    "        ):\n",
    "            gfxdraw.aapolygon(self.surf, poly, color)\n",
    "            gfxdraw.filled_polygon(self.surf, poly, color)\n",
    "\n",
    "    def _create_image_array(self, screen, size):\n",
    "        import pygame\n",
    "\n",
    "        scaled_screen = pygame.transform.smoothscale(screen, size)\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(scaled_screen)), axes=(1, 0, 2)\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            self.isopen = False\n",
    "            pygame.quit()\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     a = np.array([0.0, 0.0, 0.0])\n",
    "#     import pygame\n",
    "\n",
    "#     def register_input():\n",
    "#         for event in pygame.event.get():\n",
    "#             if event.type == pygame.KEYDOWN:\n",
    "#                 if event.key == pygame.K_LEFT:\n",
    "#                     a[0] = -1.0\n",
    "#                 if event.key == pygame.K_RIGHT:\n",
    "#                     a[0] = +1.0\n",
    "#                 if event.key == pygame.K_UP:\n",
    "#                     a[1] = +1.0\n",
    "#                 if event.key == pygame.K_DOWN:\n",
    "#                     a[2] = +0.8  # set 1.0 for wheels to block to zero rotation\n",
    "#                 if event.key == pygame.K_RETURN:\n",
    "#                     global restart\n",
    "#                     restart = True\n",
    "\n",
    "#             if event.type == pygame.KEYUP:\n",
    "#                 if event.key == pygame.K_LEFT:\n",
    "#                     a[0] = 0\n",
    "#                 if event.key == pygame.K_RIGHT:\n",
    "#                     a[0] = 0\n",
    "#                 if event.key == pygame.K_UP:\n",
    "#                     a[1] = 0\n",
    "#                 if event.key == pygame.K_DOWN:\n",
    "#                     a[2] = 0\n",
    "\n",
    "#     env = CarRacing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38376d",
   "metadata": {},
   "source": [
    "<font size=\"5\">Double DQN Agent</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e634c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Agent:\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions, memory_size, batch_size):\n",
    "        \n",
    "        self.n_observations = n_observations        # Number of observations in each state\n",
    "        self.n_actions = n_actions                  # Number of possible actions\n",
    "        self.memory = deque(maxlen = memory_size)   # Maximum memory size\n",
    "        self.batch_size = batch_size                # Batch size\n",
    "        \n",
    "        self.epsilon = 1.0                          # Initial exploration rate\n",
    "        self.epsilon_min = 0.1                      # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.9999                 # Exploration decay rate\n",
    "        \n",
    "        self.gamma = 0.95                           # Discount rate\n",
    "        self.learning_rate = 0.001                  # Learning rate\n",
    "        \n",
    "        # Main model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        #Target model\n",
    "        self.target_model = self.build_model()\n",
    "    \n",
    "    def gray_scale(self, state, shape = (96, 96)):\n",
    "        \n",
    "        \"\"\" This function gets as input the state in \n",
    "        RGB color scale and transforms it in gray-scale\n",
    "        \"\"\"\n",
    "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "        state = state.astype(float)\n",
    "        state /= 255.0\n",
    "        state = state.reshape((*shape, 1))\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        q_net = Sequential()\n",
    "        \n",
    "        if STEP >= 2:\n",
    "            q_net.add(Conv2D(filters = 6, kernel_size = (7, 7), strides = 3, activation = 'relu', input_shape = (96, 96, 1)))\n",
    "        else:\n",
    "            q_net.add(Conv2D(filters = 6, kernel_size = (7, 7), strides = 3, activation = 'relu', input_shape = (96, 96, 3)))\n",
    "        \n",
    "        if STEP >= 5:\n",
    "            q_net.add(MaxPooling2D(pool_size = (2, 2)))   \n",
    "        \n",
    "        q_net.add(Conv2D(filters = 12, kernel_size = (4, 4), activation = 'relu'))\n",
    "        \n",
    "        if STEP >= 5:\n",
    "            q_net.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "        \n",
    "        q_net.add(Flatten())\n",
    "        \n",
    "        q_net.add(Dense(216, activation = 'relu'))\n",
    "        q_net.add(Dense(64, activation = 'relu'))\n",
    "        q_net.add(Dense(self.n_actions, activation = None))\n",
    "        \n",
    "        q_net.compile(loss = 'mean_squared_error', optimizer = Adam(learning_rate = self.learning_rate, epsilon = 1e-7))\n",
    "        \n",
    "        return q_net\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \n",
    "        \"\"\" This functions gets as input a state\n",
    "        and returns the selected action based on the ε-greedy policy\n",
    "        \"\"\"\n",
    "        \n",
    "        state = tf.convert_to_tensor(state[None, :], dtype = tf.float32)\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state, verbose = 0));\n",
    "\n",
    "    def train_model(self):\n",
    "        \n",
    "        # Updates the network weights after enough data is collected\n",
    "        if self.batch_size >= len(self.memory):\n",
    "            return\n",
    "\n",
    "        # Samples a batch from the memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        state_batch, action_batch, reward_batch, upcoming_state_batch, done_batch = list(map(np.array, list(zip(*batch))))\n",
    "        \n",
    "        q_target = self.model.predict(state_batch, verbose = 0);\n",
    "        q_target_upcoming = self.model.predict(upcoming_state_batch, verbose = 0);\n",
    "        \n",
    "        q_upcoming = self.target_model(upcoming_state_batch);\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if done_batch[i]:\n",
    "                q_target[i][action_batch[i]] = reward_batch[i]\n",
    "            else:\n",
    "                index = np.argmax(q_target_upcoming[i])\n",
    "                q_target[i][action_batch[i]] = reward_batch[i] + self.gamma * q_upcoming[i][index]\n",
    "\n",
    "        self.model.fit(np.array(state_batch), np.array(q_target), batch_size = self.batch_size, verbose = 0)\n",
    "        \n",
    "        # Reduces the exploration rate if has not reached its minimum value\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return self.epsilon\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        \"\"\" This function adds experience to the replay buffer \"\"\"\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \n",
    "        \"\"\" This function updates the weights of the target model \"\"\"\n",
    "        \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def save_model(self, episode):\n",
    "        \n",
    "        \"\"\" This function saves the weights of the model during training\"\"\"\n",
    "        \n",
    "        self.model.save_weights(\"carracing-\" + str.lower(MODES[str(STEP)]) + \"-\" + str(episode) + \".h5\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = CarRacing()\n",
    "    \n",
    "    # Maximum number of training episodes\n",
    "    episodes = 1000\n",
    "    scores_epsilon = []\n",
    "    \n",
    "    state_size = env.observation_space.shape\n",
    "    action_size = 5\n",
    "    \n",
    "    if STEP >= 4:\n",
    "        skip_frames = 3  \n",
    "    else:\n",
    "        skip_frames = 0\n",
    "    \n",
    "    if STEP >= 3:\n",
    "        # Replay memory size and batch size\n",
    "        memory_size = 150000\n",
    "        batch_size = 64\n",
    "        \n",
    "    else:\n",
    "        # Replay memory size and batch size\n",
    "        memory_size = 10000\n",
    "        batch_size = 10\n",
    "        \n",
    "    \n",
    "    agent = DDQN_Agent(state_size, action_size, memory_size, batch_size)\n",
    "    \n",
    "    # Initial exploration rate\n",
    "    epsilon = 1\n",
    "\n",
    "    for game in range(episodes):\n",
    "        \n",
    "        done = False\n",
    "        counter = 0\n",
    "        negative_counter = 0\n",
    "        \n",
    "        # Total reward per episode\n",
    "        episode_score = 0\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        if STEP >= 2:\n",
    "            # Convert to gray scale\n",
    "            state = agent.gray_scale(state)\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # Renders the screen after each step\n",
    "            #env.render()\n",
    "\n",
    "            # Gets the action for a given state\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            stacked_reward = 0\n",
    "            for _ in range(skip_frames + 1):\n",
    "                \n",
    "                # Gets the feedback after implementing a specific action\n",
    "                upcoming_state, reward, done, _ = env.step(action)\n",
    "                stacked_reward += reward\n",
    "            \n",
    "                if done: \n",
    "                    break\n",
    "                \n",
    "                # Steps per episode\n",
    "                counter += 1\n",
    "            \n",
    "            if STEP >= 2:\n",
    "                # Convert the upcoming state to gray scale\n",
    "                upcoming_state = agent.gray_scale(upcoming_state)\n",
    "\n",
    "            # Add experience to the memory\n",
    "            agent.remember(state, action, reward, upcoming_state, done)\n",
    "\n",
    "            # Update the current state with the next state\n",
    "            state = upcoming_state\n",
    "                     \n",
    "            # Train the neural network\n",
    "            epsilon = agent.train_model()\n",
    "            \n",
    "            # Update episode score\n",
    "            episode_score += stacked_reward\n",
    "            \n",
    "            # Stop the game when there are many wrong moves after a specific point\n",
    "            if stacked_reward < 0:\n",
    "                negative_counter += 1\n",
    "                if negative_counter >= 100:\n",
    "                    break  \n",
    "            \n",
    "            # Stop the game if the score is too negative\n",
    "            if episode_score < -30:\n",
    "                break \n",
    "        \n",
    "        if STEP >= 6:\n",
    "            # Update target neural netwrok\n",
    "            if game % 1 == 0:\n",
    "                agent.update_target_model()\n",
    "        else:\n",
    "            # Update target neural netwrok\n",
    "            if game % 5 == 0:\n",
    "                agent.update_target_model()\n",
    "            \n",
    "        # Saves the network weights\n",
    "        if game % 20 == 0:\n",
    "            agent.save_model(game)\n",
    "        \n",
    "        scores_epsilon.append([episode_score, epsilon])\n",
    "        \n",
    "        print('Episode:', game + 1, '| Cumulative Reward: {:.2f}'.format(episode_score), \n",
    "              '| Exploration Rate: {:.5f}'.format(epsilon), '| Steps: {:.0f}'.format(counter))\n",
    "        \n",
    "    # Closes the environment\n",
    "    env.close()\n",
    "    \n",
    "    # Save the cummulative reward in each episode to a csv file\n",
    "    df = pd.DataFrame(scores_epsilon)\n",
    "    df.columns = ['S'+str(STEP)+'_Score', 'S'+str(STEP)+'_Epsilon']\n",
    "    \n",
    "    df.to_excel(MODES[str(STEP)]+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67461fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
